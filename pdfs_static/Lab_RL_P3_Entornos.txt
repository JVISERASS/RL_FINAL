4Âº iMAT                   Laboratorio de Aprendizaje por Refuerzo          Curso 2024-25




PrÃ¡ctica final â€“ AplicaciÃ³n de conceptos
generales de RL a varios entornos.
SesiÃ³n segunda. OperaciÃ³n en almacÃ©n
1. IntroducciÃ³n
En el resto del trabajo prÃ¡ctico del curso, tras habernos iniciado en el RL con
aproximaciÃ³n de funciÃ³n en un entorno muy simple, vamos a complicar
progresivamente el entorno con el que interactuamos.

Nos moveremos en el mismo recinto, con los mismos obstÃ¡culos, que ahora van a
representar estanterÃ­as donde puede haber objetos. Esto da lugar a un entorno de
almacÃ©n como el representado en la Figura 1. El agente se representa en color naranja
y la zona en verde representa la zona de entrega. Aparecen en azul tres objetos, uno por
estanterÃ­a.




                             Figura 1: El entorno almacÃ©n


2. Trabajo previo
Lee este documento y reflexiona sobre los retos que pueden plantear los distintos
entornos a un agente entrenado con RL. AsegÃºrate de que comprendes cuÃ¡les son las
acciones que puede elegir el agente en cada uno de los entornos propuestos. En base a
tus conclusiones sobre la sesiÃ³n anterior, reflexiona sobre cuÃ¡les serÃ­an diseÃ±os
adecuados de la observaciÃ³n tambiÃ©n para cada entorno.

No hay que realizar ninguna entrega, pero habrÃ¡ un tiempo dedicado a discutir el
problema al inicio de la sesiÃ³n en el que los profesores harÃ¡n preguntas sobre este
trabajo prÃ¡ctico, cuyos detalles damos por conocidos al comienzo de la sesiÃ³n.



                                          1 de 3
4Âº iMAT                             Laboratorio de Aprendizaje por Refuerzo                            Curso 2024-25



3. Entornos
Trabajaremos sobre tres variantes del entorno que son, en orden creciente de
complejidad:

     â€¢    Entorno 1: Objetos fijos, recogida de un objeto. En este caso, los objetos estÃ¡n
          en tres posiciones fijas. El agente debe aproximarse a uno cualquiera de ellos e
          implementar la recogida del objeto de forma exitosa. La acciÃ³n de entrega no
          tiene nada asociado, por lo que podemos obviarla. El episodio termina (con
          fracaso) si el agente toca las paredes del perÃ­metro o alguna estanterÃ­a y tambiÃ©n
          (con Ã©xito) en el momento en el que el agente recoge un objeto.
     â€¢    Entorno 2: Objetos fijos, recogida y entrega de un objeto. En este caso, los
          objetos estÃ¡n en tres posiciones fijas. El objetivo del agente es entregar un
          objeto en la zona de entrega. LÃ³gicamente, para ello tiene que haber cogido un
          objeto antes. El episodio termina con fracaso si el agente toca las paredes del
          perÃ­metro o alguna estanterÃ­a o si suelta el objeto fuera del Ã¡rea de recogida, y
          con Ã©xito en el momento en el que el agente suelta un objeto dentro del Ã¡rea de
          recogida (entrega correcta).
     â€¢    Entorno 3: Objetos en posiciÃ³n aleatoria, recogida y entrega de un objeto. Este
          entorno es igual que Entorno 2, pero la posiciÃ³n de los objetos en las estanterÃ­as
          cambia de episodio a episodio.

Detalles de los entornos

Como puede observarse en la Figura 1, el entorno tiene forma cuadrada con 10 metros
de lado. El Ã¡rea objetivo es un rectÃ¡ngulo cuyo vÃ©rtice inferior izquierdo se encuentra
situado en (x = 2.5, y = 9), con dimensiones (l_x = 5, l_y = 1). Por otro lado, hay tres
estanterÃ­as con dimensiones (l_x = 0.2, l_y = 5) y cuyos vÃ©rtices inferiores izquierdos se
encuentran situados respectivamente en (x = 1.9, y = 1), (x = 4.9, y = 1) y (x = 7.9, y = 1).

El entorno devuelve una observaciÃ³n con la siguiente estructura:

     â€¢    obs[0]: posiciÃ³n del agente en el eje x.
     â€¢    obs[1]: posiciÃ³n del agente en el eje y.
     â€¢    obs[2]: posiciÃ³n del objeto 1 en el eje x.
     â€¢    obs[3]: posiciÃ³n del objeto 1 en el eje y.
     â€¢    obs[4]: posiciÃ³n del objeto 2 en el eje x.
     â€¢    obs[5]: posiciÃ³n del objeto 2 en el eje y.
     â€¢    obs[6]: posiciÃ³n del objeto 3 en el eje x.
     â€¢    obs[7]: posiciÃ³n del objeto 3 en el eje y.
     â€¢    obs[8]: agent_has_object. Representa si el agente porta un objeto o no.
     â€¢    obs[9]: collision. A 1 si el agente ha chocado con pared o estanterÃ­a.
     â€¢    obs[10]: delivery. A 1 si el agente suelta un objeto en la zona de entrega.

El entorno recibe una acciÃ³n del agente que, para todos los estados, puede ser ğ’œğ’œ(ğ‘ ğ‘ ) =
{ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘, ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–, ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘â„ğ‘ğ‘, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘, ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ }. Entonces, si el agente elige una
acciÃ³n de movimiento, se desplaza 0.25 metros en la direcciÃ³n elegida; y si elige una
acciÃ³n sobre un objeto, aplica lo siguiente:
                                                        2 de 3
4Âº iMAT                       Laboratorio de Aprendizaje por Refuerzo            Curso 2024-25


   â€¢      coger: si el agente elige esta acciÃ³n y hay un objeto a menos de 60 cm, entonces
          recoge con Ã©xito dicho objeto, lo que se reflejarÃ¡ en un flanco a 1 en la variable
          agent_has_object. A partir de este momento, salvo que termine el episodio, la
          posiciÃ³n de ese objeto serÃ¡ la del agente.
   â€¢      soltar: sÃ³lo aplica a los entornos 2 y 3. En estos, hay un flanco a 0 de la variable
          agent_has_object, aunque importa poco, dado que en todos los casos (fracaso o
          Ã©xito) se termina el episodio.

4. IngenierÃ­a de variables, diseÃ±o de la recompensa y
   entrenamiento de los agentes
Todo esto lo tienes que resolver apoyÃ¡ndote en lo desarrollado en la sesiÃ³n anterior y
con los nuevos conocimientos adquiridos en las sesiones de teorÃ­a.

La Ãºnica cuestiÃ³n relevante para poner en marcha los procesos es que hay que pasar
unos argumentos al constructor de la clase WarehouseEnv() que tenÃ©is en el script
almacen_all.py que os hemos compartido. En concreto, hay que especificar dos
variables al instanciar la clase, que son auto explicativas: random_objects=True/False y
drop=True/False. La relaciÃ³n entre estas binarias y los entornos es:

   â€¢      Entorno 1: random_objects = False, drop = False
   â€¢      Entorno 2: random_objects = False, drop = True
   â€¢      Entorno 3: random_objects=True, drop = True

5. Objetivo de la prÃ¡ctica
El objetivo de esta prÃ¡ctica es funciÃ³n del entorno, y se puede definir como:
    â€¢ Entorno 1: entrenar mediante aprendizaje por refuerzo un agente que llegue en
        el menor nÃºmero de pasos posible a un objeto y lo coja.
    â€¢ Entornos 2 y 3: entrenar mediante aprendizaje por refuerzo un agente que
        entregue un objeto en el Ã¡rea de recogida en el menor nÃºmero de pasos posible.
Para ello, debes aplicar todas las tÃ©cnicas que consideres oportunas de las vistas en
clase.
Al principio de la sesiÃ³n se presentarÃ¡ una rÃºbrica aproximada con los detalles que
vamos a evaluar en la presentaciÃ³n final del trabajo, algo que deberÃ­a orientaros a la
hora de elegir entre las opciones disponibles.

6. Memoria de trabajo
Aunque este trabajo lo vamos a evaluar en la presentaciÃ³n final, debes recoger en un
cuaderno de trabajo todo lo que vayas haciendo. Eventualmente podemos pedÃ­rtelo
para comprobar alguno de los resultados/desarrollos tras la presentaciÃ³n final.




                                              3 de 3
