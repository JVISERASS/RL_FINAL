4Âº iMAT                   Laboratorio de Aprendizaje por Refuerzo        Curso 2025-2026




PrÃ¡ctica final â€“ AplicaciÃ³n de conceptos
generales de RL a varios entornos.
SesiÃ³n primera. NavegaciÃ³n con tile coding
1. IntroducciÃ³n
En esta sesiÃ³n comenzamos a trabajar de cara a la presentaciÃ³n final de la asignatura.
Para facilitar la comprensiÃ³n del paso de aprendizaje tabular a funciÃ³n de aproximaciÃ³n,
planteamos una prÃ¡ctica guiada en la que aprenderemos a modelar espacios continuos
con una discretizaciÃ³n que permite generalizar de forma razonadamente controlada la
experiencia obtenida desde una posiciÃ³n a un determinado entorno de Ã©sta: el tile
coding.

El funcionamiento de los mÃ©todos que permiten a un agente aprender con funciÃ³n de
aproximaciÃ³n puede complicarse mucho. Por ello, con la idea de que puedas
comprender con suficiente profundidad este proceso, lo vamos a simplificar
notablemente en esta primera sesiÃ³n. Por un lado, vamos a usar un entorno sencillo, en
el que solo tenemos que llegar a una zona objetivo desde una posiciÃ³n de partida
aleatoria evitando chocarnos con los obstÃ¡culos (paredes). En la Figura 1 se observa un
posible estado del entorno. El agente se representa en color naranja y la zona objetivo
en verde.




                          Figura 1: El entorno de navegaciÃ³n

Por otro lado, en parte gracias a la sencillez del entorno, vamos a usar aproximaciÃ³n
lineal de la funciÃ³n de valor de acciÃ³n con tile coding.


                                          1 de 5
4Âº iMAT                     Laboratorio de Aprendizaje por Refuerzo          Curso 2025-2026


Detalles del entorno

Planteamos una interacciÃ³n episÃ³dica que termina tanto si el agente llega a la zona
objetivo (Ã©xito) como si choca (fracaso) con alguna de las paredes del perÃ­metro o de los
obstÃ¡culos (en rojo). Por lo tanto, podremos modelar esta tarea como un MDP
episÃ³dico.

Como puede observarse en la Figura 1, el entorno tiene forma cuadrada con 10 metros
de lado. El Ã¡rea objetivo es un rectÃ¡ngulo cuyo vÃ©rtice inferior izquierdo se encuentra
situado en (x = 2.5, y = 8), con dimensiones (l_x = 1, l_y = 2). Por otro lado, hay tres
obstÃ¡culos con dimensiones (l_x = 1, l_y = 5) y cuyos vÃ©rtices inferiores izquierdos se
encuentran situados respectivamente en (x = 1.5, y = 1), (x = 4.5, y = 1) y (x = 7.5, y = 1).

El entorno recibe una acciÃ³n del agente que, para todos los estados, puede ser ğ’œ(ğ‘ ) =
{ğ‘ğ‘Ÿğ‘Ÿğ‘–ğ‘ğ‘, ğ‘ğ‘ğ‘ğ‘—ğ‘œ, ğ‘–ğ‘§ğ‘ğ‘¢ğ‘–ğ‘’ğ‘Ÿğ‘‘ğ‘, ğ‘‘ğ‘’ğ‘Ÿğ‘’ğ‘â„ğ‘}. Entonces, el agente se desplaza 0.25 metros en
la direcciÃ³n elegida. Cuando termina el desplazamiento, el script del entorno devuelve
Ãºnicamente la nueva posiciÃ³n del agente y dos variables binarias que representan si ha
habido colisiÃ³n o si se ha llegado a la zona objetivo. En ambos casos, el episodio termina
(variable terminated a True). Es importante notar que estas seÃ±ales no son la
observaciÃ³n que llega al agente. La seÃ±al tiene que ser generada procesando posiciÃ³n,
colisiÃ³n y llegada. Para ello, usaremos otro script que construye la realimentaciÃ³n de la
observaciÃ³n que llega al agente.

Detalles del agente

Lo definiremos en mÃ¡s profundidad en las secciones de descripciÃ³n del trabajo. Por
ahora, recibe esta seÃ±al de realimentaciÃ³n, la procesa y actualiza sus estimadores de la
funciÃ³n de valor de acciÃ³n en la fase de entrenamiento.

2. Trabajo previo
â€¢   Lee la secciÃ³n 9.5.4 de Sutton & Barto: Introduction to reinforcement learning,
    donde se presenta el concepto de tile coding.
â€¢   Visita: http://incompleteideas.net/tiles/tiles3.html, la web donde R. Sutton
    comparte un cÃ³digo para hacer tile coding de forma eficiente. Lee la documentaciÃ³n.
    La biblioteca se proporciona entre los ficheros de la prÃ¡ctica.
â€¢   Abre el fichero entorno_navegacion.py. Analiza la clase Navegacion. En concreto,
    observa la numeraciÃ³n de las acciones y la forma en la que el entorno devuelve su
    observaciÃ³n â€œcrudaâ€ del estado.
â€¢   Ahora ve al final del script. Observa que hay un cÃ³digo que instancia el entorno y
    toma varias acciones aleatorias. Ejecuta el fichero. VerÃ¡s que aparece una
    representaciÃ³n del entorno parecida a la de la Figura 1 pero en la que se observa el
    Ã¡rea en la que influye el agente desde una determinada posiciÃ³n. Realiza la siguiente
    prueba:
        o Prueba varios valores de nÃºmero de tiles por dimensiÃ³n (x, y) y nÃºmero de
            tilings y analiza las diferencias en el Ã¡rea de influencia del agente en funciÃ³n
            del nÃºmero de tiles y tilings.
            Recuerda que para visualizar la figura debes tener abierta la aplicaciÃ³n de
            MobaXterm.
                                            2 de 5
4Âº iMAT                    Laboratorio de Aprendizaje por Refuerzo          Curso 2025-2026


â€¢   Analiza el pseudocÃ³digo del algoritmo de semi-gradient SARSA (Sutton&Barto, pg.
    244).
â€¢   Antes de comenzar la sesiÃ³n habrÃ¡ un test evaluable con preguntas sobre este
    trabajo previo y, en general, sobre la aproximaciÃ³n lineal de funciÃ³n de valor.


3. IngenierÃ­a de variables y diseÃ±o de la recompensa
Abre el script representacion.py. En el mismo, estÃ¡ descrita la clase
FeedbackConstruction, que permite construir una pasarela entre el entorno y el agente
que se encarga de la ingenierÃ­a de variables (representaciÃ³n del estado para obtener
una representaciÃ³n que sea una seÃ±al de Markov). Observa que en el mÃ©todo de
inicializaciÃ³n hay una serie de variables que configuran algunos parÃ¡metros para el
tiling, como el escalado para que funcione la implementaciÃ³n del tile coding de Sutton
(tiles unitarios dentro de sus funciones), la hash table y sus dimensiones mÃ¡ximas en
funciÃ³n del nÃºmero de tiles y tilings. No toques esas lÃ­neas.

RecuperaciÃ³n de los tiles activos

En el mÃ©todo _get_active_tiles(.), aÃ±ade un cÃ³digo que detecte los tiles activos. Para
evitar la presencia de artefactos en el Ã¡rea de influencia del agente, desplaza los tilings
con offset 3 en el eje x y offset 1 en el eje y.

CÃ¡lculo de la recompensa

Para seguir el estÃ¡ndar de gymnasium, el cÃ¡lculo de la recompensa lo vamos a
implementar dentro de la funciÃ³n step(.) del entorno de navegaciÃ³n. Puedes integrarlo
directamente en la funciÃ³n step o extraerlo en una funciÃ³n del cÃ¡lculo de la recompensa.
Lo que prefieras.

4. Entrenamiento del agente
Abre el script agente.py. Vamos a trabajar con la clase SarsaAgent. En el mÃ©todo de
inicializaciÃ³n verÃ¡s una serie de atributos que son necesarios, y cuyos valores vamos a
controlar cuando instanciemos un objeto agente. No toques esas lÃ­neas. DespuÃ©s,
tenemos los pesos del modelo inicializados a 0, lo que es razonable para este problema.
No obstante, siÃ©ntete libre de modificar esto si te pareciera oportuno.

Es importante que notes que hemos reservado un juego de pesos para cada acciÃ³n. Esto
es: el agente reserva un vector de pesos con las dimensiones de la observaciÃ³n que llega
de la pasarela de realimentaciÃ³n para cada una de las acciones.

Por Ãºltimo, dejamos un espacio para aÃ±adir atributos que te sirvan para monitorizar el
proceso de aprendizaje.

Analiza el mÃ©todo get_action. EstÃ¡ planteado para muestrear una polÃ­tica ï¥-greedy,
permitiendo hacer que ï¥ sea el valor con el que se inicializa al instanciar al agente, y
tambiÃ©n que podamos controlarlo dinÃ¡micamente durante el proceso de


                                           3 de 5
4Âº iMAT                    Laboratorio de Aprendizaje por Refuerzo        Curso 2025-2026


entrenamiento, algo que es Ãºtil en general. Veremos a continuaciÃ³n cÃ³mo usarlo. En
todo caso, este mÃ©todo ya estÃ¡ desarrollado, no hay que tocarlo.

A continuaciÃ³n, tenemos el mÃ©todo para la actualizaciÃ³n de pesos (update(.)). Ã‰ste, se
apoya en el mÃ©todo de cÃ¡lculo de los valores de ğ‘, que tambiÃ©n debes resolver tÃº, en la
funciÃ³n get_q_values(.). En update(.), te damos calculado el error TD. Ahora, tÃº debes
aÃ±adir las lÃ­neas de cÃ³digo para actualizar los pesos de la funciÃ³n de aproximaciÃ³n de ğ‘.
Recuerda que los pesos son un atributo del agente. Usa la fÃ³rmula de actualizaciÃ³n con
aproximaciÃ³n lineal.

DespuÃ©s verÃ¡s el mÃ©todo train(.). Este mÃ©todo estÃ¡ ya desarrollado, pero tienes 3
hiperparÃ¡metros con los que jugar, que bÃ¡sicamente modulan la exploraciÃ³n. Se ofrece
ya programada una estrategia en la que se explora con ï¥ inicial hasta un determinado
porcentaje del nÃºmero total de episodios del entrenamiento y a partir de ahÃ­ se reduce
exponencialmente ï¥ hasta un valor mÃ­nimo. NOTA: Los valores de los tres
hiperparÃ¡metros son deliberadamente malos, por lo que tendrÃ¡s que rediseÃ±arlos.
AsegÃºrate de que el agente estÃ¡ suficiente tiempo explorando con un ï¥ bajo al final del
entrenamiento para que la polÃ­tica cuyos valores aproxima el agente se parezca a una
polÃ­tica determinista, que es como vamos a evaluarla.

TambiÃ©n puedes cambiar la frecuencia con la que muestras resultados parciales del
entrenamiento. AdemÃ¡s, en este mÃ©todo tendrÃ¡s que actualizar todos los atributos que
hayas definido para monitorizar el entrenamiento (nÃºmero de veces que se ha
alcanzado un par estado-acciÃ³n, longitud de las trayectorias, valores de algunos estados
prefijados, etc.).

Finalmente, la clase presenta el mÃ©todo evaluate(.), en el que se simulan episodios
moviendo al agente por el entorno con su estimaciÃ³n de ğ‘.

5. EvaluaciÃ³n de los agentes
Como habrÃ¡s observado en el trabajo previo, el diseÃ±o del tile coding puede impactar
en el aprendizaje del agente. Para que practicando comprendas mejor los equilibrios
entre particularizaciÃ³n y generalizaciÃ³n de la experiencia, vamos a medir el rendimiento
de dos tipos de agente: 1) un agente entrenado con sÃ³lo 10000 episodios de experiencia
y 2) un agente entrenado con toda la experiencia que consideres oportuna.
En ambos casos, el banco de ensayos constarÃ¡ de 1000 episodios con punto de partida
pseudoaleatorio (por lo tanto, el mismo para todos los agentes).
La evaluaciÃ³n la haremos los profesores (no conocerÃ¡s esas posiciones iniciales) una sola
vez, con los dos agentes que nos envÃ­es.

6. Opcional
Modifica la clase SarsaAgent para convertirla QLearningAgent, de manera que un agente
que se apoye en ella aprenda usando Q-learning. Usa tambiÃ©n de momento
aproximaciÃ³n lineal.




                                           4 de 5
4Âº iMAT                  Laboratorio de Aprendizaje por Refuerzo      Curso 2025-2026



7. Memoria de trabajo
Aunque este trabajo lo vamos a evaluar en la presentaciÃ³n final, debes recoger en un
cuaderno de trabajo todo lo que vayas haciendo. Eventualmente podemos pedÃ­rtelo
para comprobar alguno de los resultados/desarrollos tras la presentaciÃ³n final.




                                         5 de 5
